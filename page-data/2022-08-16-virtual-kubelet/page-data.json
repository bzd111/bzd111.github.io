{"componentChunkName":"component---src-templates-blog-post-js","path":"/2022-08-16-virtual-kubelet/","result":{"data":{"markdownRemark":{"html":"<!-- vim-markdown-toc GitLab -->\n<ul>\n<li><a href=\"#%E5%89%8D%E8%A8%80\">前言</a></li>\n<li><a href=\"#kubelet\">Kubelet</a></li>\n<li><a href=\"#virtual-kubelet\">Virtual Kubelet</a>\n<ul>\n<li><a href=\"#eci\">ECI</a></li>\n</ul>\n</li>\n<li><a href=\"#%E9%99%90%E5%88%B6\">限制</a></li>\n<li><a href=\"#reference\">Reference</a>\n<ul>\n<li><a href=\"#-httpsgithubcomvirtual-kubeletalibabacloud-eciblobmasterecigo\">- https://github.com/virtual-kubelet/alibabacloud-eci/blob/master/eci.go</a></li>\n</ul>\n</li>\n</ul>\n<!-- vim-markdown-toc -->\n<h1 id=\"前言\" style=\"position:relative;\"><a href=\"#%E5%89%8D%E8%A8%80\" aria-label=\"前言 permalink\" class=\"toc-anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>前言</h1>\n<p>[Azure Container Instances (ACI)]\n最近写 Operator 的时候，由于资源问题，需要把 cr 调度到，阿里云的<a href=\"https://help.aliyun.com/document_detail/89129.html\">ECI(Elastic Container Instance)</a>上，ECI 的底层是使用<code class=\"language-text\">Virtual Kubelet</code>。</p>\n<p>这里稍作记录。</p>\n<h1 id=\"kubelet\" style=\"position:relative;\"><a href=\"#kubelet\" aria-label=\"kubelet permalink\" class=\"toc-anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Kubelet</h1>\n<p>kubelet 作为 Node 上一个重要的组件，调用 CRI，CNI，CSI 来创建相应的 Pod，并定期执行 Probe，那 Virtual Kubelet 是怎么实现的呢。</p>\n<h1 id=\"virtual-kubelet\" style=\"position:relative;\"><a href=\"#virtual-kubelet\" aria-label=\"virtual kubelet permalink\" class=\"toc-anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Virtual Kubelet</h1>\n<p>官方定义：Virtual Kubelet is an open-source Kubernetes kubelet implementation that masquerades as a kubelet.</p>\n<p>很多云厂商都提供了这个实现，代码都在<a href=\"https://github.com/virtual-kubelet\">这个组织下</a></p>\n<ul>\n<li><a href=\"https://github.com/admiraltyio/multicluster-scheduler/blob/master/README.md#readme\">Admiralty Multi-Cluster Scheduler</a></li>\n<li><a href=\"https://github.com/admiraltyio/multicluster-scheduler/blob/master/README.md#readme\">Alibaba Cloud Elastic Container Instance (ECI)</a></li>\n<li><a href=\"https://github.com/virtual-kubelet/aws-fargate/blob/master/README.md#readme\">AWS Fargate</a></li>\n<li><a href=\"https://github.com/virtual-kubelet/azure-batch/blob/master/README.md#readme\">Azure Batch</a></li>\n<li><a href=\"https://github.com/virtual-kubelet/azure-aci/blob/master/README.md#readme\">Azure Container Instances (ACI)</a></li>\n<li><a href=\"https://github.com/elotl/kip/blob/master/README.md#readme\">Elotl Kip</a></li>\n<li><a href=\"https://github.com/virtual-kubelet/cri/blob/master/README.md#readme\">Kubernetes Container Runtime Interface (CRI)</a></li>\n<li><a href=\"https://github.com/virtual-kubelet/huawei-cci/blob/master/README.md#readme\">Huawei Cloud Container Instance (CCI)</a></li>\n<li><a href=\"https://github.com/virtual-kubelet/nomad/blob/master/README.md#readme\">HashiCorp Nomad</a></li>\n<li><a href=\"https://github.com/liqotech/liqo/blob/master/README.md#readme\">Liqo</a></li>\n<li><a href=\"https://github.com/virtual-kubelet/openstack-zun/blob/master/README.md#readme\">OpenStack Zun</a></li>\n<li><a href=\"https://github.com/virtual-kubelet/tensile-kube/blob/master/README.md#readme\">Tencent Games Tensile Kube</a></li>\n</ul>\n<h2 id=\"eci\" style=\"position:relative;\"><a href=\"#eci\" aria-label=\"eci permalink\" class=\"toc-anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>ECI</h2>\n<p>这里重点看下 Aliyun ECI 的开源实现版本。</p>\n<p>在 main.go 里，创建 rootCmd，rootCmd 的 RunE func 是 runRootCommand，</p>\n<div class=\"gatsby-highlight\" data-language=\"golang\"><pre class=\"language-golang\"><code class=\"language-golang\">\nfunc NewCommand(ctx context.Context, name string, c Opts) *cobra.Command {\n\tcmd := &amp;cobra.Command{\n\t\tUse:   name,\n\t\tShort: name + &quot; provides a virtual kubelet interface for your kubernetes cluster.&quot;,\n\t\tLong: name + ` implements the Kubelet interface with a pluggable\nbackend implementation allowing users to create kubernetes nodes without running the kubelet.\nThis allows users to schedule kubernetes workloads on nodes that aren&#39;t running Kubernetes.`,\n\t\tRunE: func(cmd *cobra.Command, args []string) error {\n\t\t\treturn runRootCommand(ctx, c)\n\t\t},\n\t}\n\n\tinstallFlags(cmd.Flags(), &amp;c)\n\treturn cmd\n}</code></pre></div>\n<p>runRootCommand 创建了，podInformer、secretInformer、configMapInformer、serviceInformer 四个 informer，同时将这四个 informer 传到 manager.NewResourceManager 里</p>\n<div class=\"gatsby-highlight\" data-language=\"golang\"><pre class=\"language-golang\"><code class=\"language-golang\">func runRootCommand(ctx context.Context, c Opts) error {\n    ......\n\n\t// Create a shared informer factory for Kubernetes pods in the current namespace (if specified) and scheduled to the current node.\n\tpodInformerFactory := kubeinformers.NewSharedInformerFactoryWithOptions(\n\t\tclient,\n\t\tc.InformerResyncPeriod,\n\t\tkubeinformers.WithNamespace(c.KubeNamespace),\n\t\tkubeinformers.WithTweakListOptions(func(options *metav1.ListOptions) {\n\t\t\toptions.FieldSelector = fields.OneTermEqualSelector(&quot;spec.nodeName&quot;, c.NodeName).String()\n\t\t}))\n\tpodInformer := podInformerFactory.Core().V1().Pods()\n\n\t// Create another shared informer factory for Kubernetes secrets and configmaps (not subject to any selectors).\n\tscmInformerFactory := kubeinformers.NewSharedInformerFactoryWithOptions(client, c.InformerResyncPeriod)\n\t// Create a secret informer and a config map informer so we can pass their listers to the resource manager.\n\tsecretInformer := scmInformerFactory.Core().V1().Secrets()\n\tconfigMapInformer := scmInformerFactory.Core().V1().ConfigMaps()\n\tserviceInformer := scmInformerFactory.Core().V1().Services()\n\n\tgo podInformerFactory.Start(ctx.Done())\n\tgo scmInformerFactory.Start(ctx.Done())\n\n\trm, err := manager.NewResourceManager(podInformer.Lister(), secretInformer.Lister(), configMapInformer.Lister(), serviceInformer.Lister())\n\n\tp, err := alibabacloud.NewECIProvider(\n\t\tc.ProviderConfigPath,\n\t\trm,\n\t\tc.NodeName,\n\t\tc.OperatingSystem,\n\t\tos.Getenv(&quot;VKUBELET_POD_IP&quot;),\n\t\tc.ListenPort,\n\t)\n\n\tvar leaseClient v1beta1.LeaseInterface\n\tif c.EnableNodeLease {\n\t\tleaseClient = client.CoordinationV1beta1().Leases(corev1.NamespaceNodeLease)\n\t}\n\n\tpNode := NodeFromProvider(ctx, c.NodeName, taint, p, c.Version)\n\tnodeRunner, err := node.NewNodeController(\n\t\tnode.NaiveNodeProvider{},\n\t\tpNode,\n\t\tclient.CoreV1().Nodes(),\n\t\tnode.WithNodeEnableLeaseV1Beta1(leaseClient, nil),\n\t\tnode.WithNodeStatusUpdateErrorHandler(func(ctx context.Context, err error) error {\n\t\t\tif !k8serrors.IsNotFound(err) {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tnewNode := pNode.DeepCopy()\n\t\t\tnewNode.ResourceVersion = &quot;&quot;\n\t\t\t_, err = client.CoreV1().Nodes().Create(newNode)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}),\n\t)\n\n\teb := record.NewBroadcaster()\n\teb.StartLogging(log.G(ctx).Infof)\n\teb.StartRecordingToSink(&amp;corev1client.EventSinkImpl{Interface: client.CoreV1().Events(c.KubeNamespace)})\n\n\tpc, err := node.NewPodController(node.PodControllerConfig{\n\t\tPodClient:       client.CoreV1(),\n\t\tPodInformer:     podInformer,\n\t\tEventRecorder:   eb.NewRecorder(scheme.Scheme, corev1.EventSource{Component: path.Join(pNode.Name, &quot;pod-controller&quot;)}),\n\t\tProvider:        p,\n\t\tSecretLister:    secretInformer.Lister(),\n\t\tConfigMapLister: configMapInformer.Lister(),\n\t\tServiceLister:   serviceInformer.Lister(),\n\t})\n\n\tcancelHTTP, err := setupHTTPServer(ctx, p, apiConfig)\n\tdefer cancelHTTP()\n\n\tgo func() {\n\t\tif err := pc.Run(ctx, c.PodSyncWorkers); err != nil &amp;&amp; errors.Cause(err) != context.Canceled {\n\t\t\tlog.G(ctx).Fatal(err)\n\t\t}\n\t}()\n\n\tgo func() {\n\t\tif err := nodeRunner.Run(ctx); err != nil {\n\t\t\tlog.G(ctx).Fatal(err)\n\t\t}\n\t}()\n\n\t&lt;-ctx.Done()\n\treturn nil\n}</code></pre></div>\n<p>ResourceManager 是一个结构体，属性是 4 个 Lister Interface，针对不对的资源，提供了 4 个相应的 method</p>\n<div class=\"gatsby-highlight\" data-language=\"golang\"><pre class=\"language-golang\"><code class=\"language-golang\">type ResourceManager struct {\n\tpodLister       corev1listers.PodLister\n\tsecretLister    corev1listers.SecretLister\n\tconfigMapLister corev1listers.ConfigMapLister\n\tserviceLister   corev1listers.ServiceLister\n}\n\n// GetPods returns a list of all known pods assigned to this virtual node.\nfunc (rm *ResourceManager) GetPods() []*v1.Pod {\n\tl, err := rm.podLister.List(labels.Everything())\n\tif err == nil {\n\t\treturn l\n\t}\n\tlog.L.Errorf(&quot;failed to fetch pods from lister: %v&quot;, err)\n\treturn make([]*v1.Pod, 0)\n}\n\n// GetConfigMap retrieves the specified config map from the cache.\nfunc (rm *ResourceManager) GetConfigMap(name, namespace string) (*v1.ConfigMap, error) {\n\treturn rm.configMapLister.ConfigMaps(namespace).Get(name)\n}\n\n// GetSecret retrieves the specified secret from Kubernetes.\nfunc (rm *ResourceManager) GetSecret(name, namespace string) (*v1.Secret, error) {\n\treturn rm.secretLister.Secrets(namespace).Get(name)\n}\n\n// ListServices retrieves the list of services from Kubernetes.\nfunc (rm *ResourceManager) ListServices() ([]*v1.Service, error) {\n\treturn rm.serviceLister.List(labels.Everything())\n}\n</code></pre></div>\n<p>NewECIProvider 创建了一个 ECIProvider 的实例，它实现了 virtual-kubelet 的 PodLifecycleHandler Interface，</p>\n<p>通过环境变量传入了很多参数，这些参数在阿里云控制台安装 ack-virtual-node 组件，需要填入的，有些是必填，有些是可选的。</p>\n<p>rm 就是上文中创建的 ResourceManager，</p>\n<div class=\"gatsby-highlight\" data-language=\"golang\"><pre class=\"language-golang\"><code class=\"language-golang\">// PodLifecycleHandler defines the interface used by the PodController to react\n// to new and changed pods scheduled to the node that is being managed.\n//\n// Errors produced by these methods should implement an interface from\n// github.com/virtual-kubelet/virtual-kubelet/errdefs package in order for the\n// core logic to be able to understand the type of failure.\ntype PodLifecycleHandler interface {\n\t// CreatePod takes a Kubernetes Pod and deploys it within the provider.\n\tCreatePod(ctx context.Context, pod *corev1.Pod) error\n\n\t// UpdatePod takes a Kubernetes Pod and updates it within the provider.\n\tUpdatePod(ctx context.Context, pod *corev1.Pod) error\n\n\t// DeletePod takes a Kubernetes Pod and deletes it from the provider.\n\tDeletePod(ctx context.Context, pod *corev1.Pod) error\n\n\t// GetPod retrieves a pod by name from the provider (can be cached).\n\tGetPod(ctx context.Context, namespace, name string) (*corev1.Pod, error)\n\n\t// GetPodStatus retrieves the status of a pod by name from the provider.\n\tGetPodStatus(ctx context.Context, namespace, name string) (*corev1.PodStatus, error)\n\n\t// GetPods retrieves a list of all pods running on the provider (can be cached).\n\tGetPods(context.Context) ([]*corev1.Pod, error)\n}\n\nfunc NewECIProvider(config string, rm *manager.ResourceManager, nodeName, operatingSystem string, internalIP string, daemonEndpointPort int32) (*ECIProvider, error) {\n\tvar p ECIProvider\n\tvar err error\n\tp.resourceManager = rm\n\tp.clusterName = os.Getenv(&quot;ECI_CLUSTER_NAME&quot;)\n\tp.region = os.Getenv(&quot;ECI_REGION&quot;)\n\taccessKey  := os.Getenv(&quot;ECI_ACCESS_KEY&quot;)\n\tsecretKey := os.Getenv(&quot;ECI_SECRET_KEY&quot;)\n\tp.secureGroup == os.Getenv(&quot;ECI_SECURITY_GROUP&quot;)\n\tp.vSwitch= os.Getenv(&quot;ECI_VSWITCH&quot;)\n\n\tp.eciClient, err = eci.NewClientWithAccessKey(p.region, accessKey, secretKey)\n\tp.cpu = &quot;1000&quot;\n\tp.memory = &quot;4Ti&quot;\n\tp.pods = &quot;1000&quot;\n\n\tp.cpu = os.Getenv(&quot;ECI_QUOTA_CPU&quot;)\n\tp.memory= os.Getenv(&quot;ECI_QUOTA_MEMORY&quot;)\n\n\tp.pods= os.Getenv(&quot;ECI_QUOTA_POD&quot;)\n\tp.operatingSystem = operatingSystem\n\tp.nodeName = nodeName\n\tp.internalIP = internalIP\n\tp.daemonEndpointPort = daemonEndpointPort\n\treturn &amp;p, err\n}\n</code></pre></div>\n<p>接下来看下，ECIProvider 是如何实现 PodLifecycleHandler Interface 的</p>\n<p>CreatePod 调用 eci 的 CreateContainerGroupRequest 请求，把 pod 转换成 Container Group</p>\n<p>DeletePod 调用 eci 的 CreateDescribeContainerGroupsRequest 查询到 ContainerGroup，然后在调用 CreateDeleteContainerGroupRequest 请求</p>\n<p>GetPods 调用 eci 的 CreateDescribeContainerGroupsRequest 请求，把 Container Group 转化成 pod</p>\n<p>GetPod 调用 GetPods 返回所有 Pod，然后 namespace, name 遍历 filter 对应的 Pod</p>\n<p>GetPodStatus 只是返回 GetPod 的 status 部分</p>\n<p>它还实现了 GetContainerLogs method，调用 CreateDescribeContainerLogRequest 请求取回日志</p>\n<p>请求阿里云相关的代码，都在 eci 目录下。</p>\n<div class=\"gatsby-highlight\" data-language=\"golang\"><pre class=\"language-golang\"><code class=\"language-golang\">// \tCreatePod(ctx context.Context, pod *corev1.Pod) error\n// 解析Pod, 组装CreateContainerGroupRequest，这两个struct和Pod里定义非常相似。\ntype CreateContainerGroupRequest struct {\n\t*requests.RpcRequest\n\tContainers               []CreateContainer         `position:&quot;Query&quot; name:&quot;Container&quot;  type:&quot;Repeated&quot;`\n\tInitContainers           []CreateContainer         `position:&quot;Query&quot; name:&quot;InitContainer&quot;  type:&quot;Repeated&quot;`\n\tResourceOwnerId          requests.Integer          `position:&quot;Query&quot; name:&quot;ResourceOwnerId&quot;`\n\tSecurityGroupId          string                    `position:&quot;Query&quot; name:&quot;SecurityGroupId&quot;`\n\tImageRegistryCredentials []ImageRegistryCredential `position:&quot;Query&quot; name:&quot;ImageRegistryCredential&quot;  type:&quot;Repeated&quot;`\n\tTags                     []Tag                     `position:&quot;Query&quot; name:&quot;Tag&quot;  type:&quot;Repeated&quot;`\n\tResourceOwnerAccount     string                    `position:&quot;Query&quot; name:&quot;ResourceOwnerAccount&quot;`\n\tRestartPolicy            string                    `position:&quot;Query&quot; name:&quot;RestartPolicy&quot;`\n\tOwnerAccount             string                    `position:&quot;Query&quot; name:&quot;OwnerAccount&quot;`\n\tOwnerId                  requests.Integer          `position:&quot;Query&quot; name:&quot;OwnerId&quot;`\n\tVSwitchId                string                    `position:&quot;Query&quot; name:&quot;VSwitchId&quot;`\n\tVolumes                  []Volume                  `position:&quot;Query&quot; name:&quot;Volume&quot;  type:&quot;Repeated&quot;`\n\tContainerGroupName       string                    `position:&quot;Query&quot; name:&quot;ContainerGroupName&quot;`\n\tZoneId                   string                    `position:&quot;Query&quot; name:&quot;ZoneId&quot;`\n}\n\ntype CreateContainer struct {\n\tName            string           `name:&quot;Name&quot;`\n\tImage           string           `name:&quot;Image&quot;`\n\tMemory          requests.Float   `name:&quot;Memory&quot;`\n\tCpu             requests.Float   `name:&quot;Cpu&quot;`\n\tWorkingDir      string           `name:&quot;WorkingDir&quot;`\n\tImagePullPolicy string           `name:&quot;ImagePullPolicy&quot;`\n\tCommands        []string         `name:&quot;Command&quot;  type:&quot;Repeated&quot;`\n\tArgs            []string         `name:&quot;Arg&quot;  type:&quot;Repeated&quot;`\n\tVolumeMounts    []VolumeMount    `name:&quot;VolumeMount&quot;  type:&quot;Repeated&quot;`\n\tPorts           []ContainerPort  `name:&quot;Port&quot;  type:&quot;Repeated&quot;`\n\tEnvironmentVars []EnvironmentVar `name:&quot;EnvironmentVar&quot;  type:&quot;Repeated&quot;`\n}\n\nfunc (p *ECIProvider) CreatePod(ctx context.Context, pod *v1.Pod) error {\n\t//Ignore daemonSet Pod\n\trequest := eci.CreateCreateContainerGroupRequest()\n\trequest.RestartPolicy = string(pod.Spec.RestartPolicy)\n\n\tcontainers, err := p.getContainers(pod, false)\n\tinitContainers, err := p.getContainers(pod, true)\n\n\t// get registry creds\n\tcreds, err := p.getImagePullSecrets(pod)\n\n\t// get volumes\n\tvolumes, err := p.getVolumes(pod)\n\n\t// assign all the things\n\trequest.Containers = containers\n\trequest.InitContainers = initContainers\n\trequest.Volumes = volumes\n\trequest.ImageRegistryCredentials = creds\n\tCreationTimestamp := pod.CreationTimestamp.UTC().Format(podTagTimeFormat)\n\ttags := []eci.Tag{\n\t\teci.Tag{Key: &quot;ClusterName&quot;, Value: p.clusterName},\n\t\teci.Tag{Key: &quot;NodeName&quot;, Value: p.nodeName},\n\t\teci.Tag{Key: &quot;NameSpace&quot;, Value: pod.Namespace},\n\t\teci.Tag{Key: &quot;PodName&quot;, Value: pod.Name},\n\t\teci.Tag{Key: &quot;UID&quot;, Value: string(pod.UID)},\n\t\teci.Tag{Key: &quot;CreationTimestamp&quot;, Value: CreationTimestamp},\n\t}\n\n\tContainerGroupName := containerGroupName(pod)\n\trequest.Tags = tags\n\trequest.SecurityGroupId = p.secureGroup\n\trequest.VSwitchId = p.vSwitch\n\trequest.ContainerGroupName = ContainerGroupName\n\tmsg := fmt.Sprintf(&quot;CreateContainerGroup request %+v&quot;, request)\n\tresponse, err := p.eciClient.CreateContainerGroup(request)\n\tmsg = fmt.Sprintf(&quot;CreateContainerGroup successed. %s, %s, %s&quot;, response.RequestId, response.ContainerGroupId, ContainerGroupName)\n\treturn nil\n}\n\n// UpdatePod 没有实现\nfunc (p *ECIProvider) UpdatePod(ctx context.Context, pod *v1.Pod) error {\n\treturn nil\n}\n\n// DeletePod deletes the specified pod out of ECI.\nfunc (p *ECIProvider) DeletePod(ctx context.Context, pod *v1.Pod) error {\n\teciId := &quot;&quot;\n\tfor _, cg := range p.GetCgs() {\n\t\tif getECITagValue(&amp;cg, &quot;PodName&quot;) == pod.Name &amp;&amp; getECITagValue(&amp;cg, &quot;NameSpace&quot;) == pod.Namespace {\n\t\t\teciId = cg.ContainerGroupId\n\t\t\tbreak\n\t\t}\n\t}\n\tif eciId == &quot;&quot; {\n\t\treturn errdefs.NotFoundf(&quot;DeletePod can&#39;t find Pod %s-%s&quot;, pod.Namespace, pod.Name)\n\t}\n\n\trequest := eci.CreateDeleteContainerGroupRequest()\n\trequest.ContainerGroupId = eciId\n\t_, err := p.eciClient.DeleteContainerGroup(request)\n\treturn wrapError(err)\n}\n\n// GetPod returns a pod by name that is running inside ECI\n// returns nil if a pod by that name is not found.\nfunc (p *ECIProvider) GetPod(ctx context.Context, namespace, name string) (*v1.Pod, error) {\n\tpods, err := p.GetPods(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tfor _, pod := range pods {\n\t\tif pod.Name == name &amp;&amp; pod.Namespace == namespace {\n\t\t\treturn pod, nil\n\t\t}\n\t}\n\treturn nil, nil\n}\n\n</code></pre></div>\n<p>接着实例化了一个 node.NewNodeController，并且通过 goroutine 启动起来</p>\n<p>NodeFromProvider 定义 v1.Node 结构体，</p>\n<div class=\"gatsby-highlight\" data-language=\"golang\"><pre class=\"language-golang\"><code class=\"language-golang\">func NodeFromProvider(ctx context.Context, name string, taint *v1.Taint, p providers.Provider, version string) *v1.Node {\n\ttaints := make([]v1.Taint, 0)\n\n\tif taint != nil {\n\t\ttaints = append(taints, *taint)\n\t}\n\n\tnode := &amp;v1.Node{\n\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\tName: name,\n\t\t\tLabels: map[string]string{\n\t\t\t\t&quot;type&quot;:                   &quot;virtual-kubelet&quot;,\n\t\t\t\t&quot;kubernetes.io/role&quot;:     &quot;agent&quot;,\n\t\t\t\t&quot;beta.kubernetes.io/os&quot;:  strings.ToLower(p.OperatingSystem()),\n\t\t\t\t&quot;kubernetes.io/hostname&quot;: name,\n\t\t\t\t&quot;alpha.service-controller.kubernetes.io/exclude-balancer&quot;: &quot;true&quot;,\n\t\t\t},\n\t\t},\n\t\tSpec: v1.NodeSpec{\n\t\t\tTaints: taints,\n\t\t},\n\t\tStatus: v1.NodeStatus{\n\t\t\tNodeInfo: v1.NodeSystemInfo{\n\t\t\t\tOperatingSystem: p.OperatingSystem(),\n\t\t\t\tArchitecture:    &quot;amd64&quot;,\n\t\t\t\tKubeletVersion:  version,\n\t\t\t},\n\t\t\tCapacity:        p.Capacity(ctx),\n\t\t\tAllocatable:     p.Capacity(ctx),\n\t\t\tConditions:      p.NodeConditions(ctx),\n\t\t\tAddresses:       p.NodeAddresses(ctx),\n\t\t\tDaemonEndpoints: *p.NodeDaemonEndpoints(ctx),\n\t\t},\n\t}\n\treturn node\n}\npNode := NodeFromProvider(ctx, c.NodeName, taint, p, c.Version)\n\n\n\nnodeRunner, err := node.NewNodeController(\n    node.NaiveNodeProvider{},\n    pNode,\n    client.CoreV1().Nodes(),\n    node.WithNodeEnableLeaseV1Beta1(leaseClient, nil),\n    node.WithNodeStatusUpdateErrorHandler(func(ctx context.Context, err error) error {\n        if !k8serrors.IsNotFound(err) {\n            return err\n        }\n\n        log.G(ctx).Debug(&quot;node not found&quot;)\n        newNode := pNode.DeepCopy()\n        newNode.ResourceVersion = &quot;&quot;\n        _, err = client.CoreV1().Nodes().Create(newNode)\n        if err != nil {\n            return err\n        }\n        log.G(ctx).Debug(&quot;created new node&quot;)\n        return nil\n    }),\n)\n\ngo func() {\n    if err := nodeRunner.Run(ctx); err != nil {\n        log.G(ctx).Fatal(err)\n    }\n}()\n\n// 定义了两个Interval，pingInterval statusInterval，下面control会用到\nfunc (n *NodeController) Run(ctx context.Context) error {\n\tif n.pingInterval == time.Duration(0) {\n\t\tn.pingInterval = DefaultPingInterval\n\t}\n\tif n.statusInterval == time.Duration(0) {\n\t\tn.statusInterval = DefaultStatusUpdateInterval\n\t}\n\n\tn.chStatusUpdate = make(chan *corev1.Node)\n\tn.p.NotifyNodeStatus(ctx, func(node *corev1.Node) {\n\t\tn.chStatusUpdate &lt;- node\n\t})\n\n\tif err := n.ensureNode(ctx); err != nil {\n\t\treturn err\n\t}\n\n\tif n.leases == nil {\n\t\tn.disableLease = true\n\t\treturn n.controlLoop(ctx)\n\t}\n\n\tn.lease = newLease(n.lease)\n\tsetLeaseAttrs(n.lease, n.n, n.pingInterval*5)\n\n\tl, err := ensureLease(ctx, n.leases, n.lease)\n\tn.lease = l\n\n\treturn n.controlLoop(ctx)\n}\n\n// 最终是执行的controlLoop，如果k8s支持node lease直接更新lease，否则就更新node status\nfunc (n *NodeController) controlLoop(ctx context.Context) error {\n\tpingTimer := time.NewTimer(n.pingInterval)\n\tdefer pingTimer.Stop()\n\n\tstatusTimer := time.NewTimer(n.statusInterval)\n\tdefer statusTimer.Stop()\n\tif n.disableLease {\n\t\t// hack to make sure this channel always blocks since we won&#39;t be using it\n\t\tif !statusTimer.Stop() {\n\t\t\t&lt;-statusTimer.C\n\t\t}\n\t}\n\n\tclose(n.chReady)\n\n\tfor {\n\t\tselect {\n\t\tcase &lt;-ctx.Done():\n\t\t\treturn nil\n\t\tcase updated := &lt;-n.chStatusUpdate:\n\t\t\tvar t *time.Timer\n\t\t\tif n.disableLease {\n\t\t\t\tt = pingTimer\n\t\t\t} else {\n\t\t\t\tt = statusTimer\n\t\t\t}\n\n\t\t\tlog.G(ctx).Debug(&quot;Received node status update&quot;)\n\t\t\t// Performing a status update so stop/reset the status update timer in this\n\t\t\t// branch otherwise there could be an uneccessary status update.\n\t\t\tif !t.Stop() {\n\t\t\t\t&lt;-t.C\n\t\t\t}\n\n\t\t\tn.n.Status = updated.Status\n\t\t\tif err := n.updateStatus(ctx, false); err != nil {\n\t\t\t\tlog.G(ctx).WithError(err).Error(&quot;Error handling node status update&quot;)\n\t\t\t}\n\t\t\tt.Reset(n.statusInterval)\n\t\tcase &lt;-statusTimer.C:\n\t\t\tif err := n.updateStatus(ctx, false); err != nil {\n\t\t\t\tlog.G(ctx).WithError(err).Error(&quot;Error handling node status update&quot;)\n\t\t\t}\n\t\t\tstatusTimer.Reset(n.statusInterval)\n\t\tcase &lt;-pingTimer.C:\n\t\t\tif err := n.handlePing(ctx); err != nil {\n\t\t\t\tlog.G(ctx).WithError(err).Error(&quot;Error while handling node ping&quot;)\n\t\t\t} else {\n\t\t\t\tlog.G(ctx).Debug(&quot;Successful node ping&quot;)\n\t\t\t}\n\t\t\tpingTimer.Reset(n.pingInterval)\n\t\t}\n\t}\n}\n</code></pre></div>\n<p>然后又启动了一个 PodController，然后把它 Run 起来了。</p>\n<p>然后创建了两个 Queue，分别用来 controller Pod 和 PodStatus，重点看下 Pod Queue</p>\n<p>AddEventHandler 把 AddFunc/UpdateFunc/DeleteFunc，放入到 queue 里，然后通过 runWorker 去消费，真正消费的函数是 syncHandler</p>\n<p>deletePod 会调用 pc.provider.DeletePod 去删除</p>\n<p>pc.createOrUpdatePod 会调用 pc.provider.GetPod 判断有没有 pod，没有的话创建，有的话就更新</p>\n<div class=\"gatsby-highlight\" data-language=\"golang\"><pre class=\"language-golang\"><code class=\"language-golang\">pc, err := node.NewPodController(node.PodControllerConfig{\n    PodClient:       client.CoreV1(),\n    PodInformer:     podInformer,\n    EventRecorder:   eb.NewRecorder(scheme.Scheme, corev1.EventSource{Component: path.Join(pNode.Name, &quot;pod-controller&quot;)}),\n    Provider:        p,\n    SecretLister:    secretInformer.Lister(),\n    ConfigMapLister: configMapInformer.Lister(),\n    ServiceLister:   serviceInformer.Lister(),\n})\n\nif err := pc.Run(ctx, c.PodSyncWorkers); err != nil &amp;&amp; errors.Cause(err) != context.Canceled {\n        log.G(ctx).Fatal(err)\n}\n\nfunc (pc *PodController) Run(ctx context.Context, podSyncWorkers int) error {\n    k8sQ := workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), &quot;syncPodsFromKubernetes&quot;)\n\tdefer k8sQ.ShutDown()\n\n\tpodStatusQueue := workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), &quot;syncPodStatusFromProvider&quot;)\n\tpc.runProviderSyncWorkers(ctx, podStatusQueue, podSyncWorkers)\n\tpc.runSyncFromProvider(ctx, podStatusQueue)\n\tdefer podStatusQueue.ShutDown()\n\n\t// Set up event handlers for when Pod resources change.\n\tpc.podsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{\n\t\tAddFunc: func(pod interface{}) {\n\t\t\tif key, err := cache.MetaNamespaceKeyFunc(pod); err != nil {\n\t\t\t\tlog.L.Error(err)\n\t\t\t} else {\n\t\t\t\tk8sQ.AddRateLimited(key)\n\t\t\t}\n\t\t},\n\t\tUpdateFunc: func(oldObj, newObj interface{}) {\n\t\t\toldPod := oldObj.(*corev1.Pod).DeepCopy()\n\t\t\tnewPod := newObj.(*corev1.Pod).DeepCopy()\n\t\t\tnewPod.ResourceVersion = oldPod.ResourceVersion\n\t\t\tif reflect.DeepEqual(oldPod.ObjectMeta, newPod.ObjectMeta) &amp;&amp; reflect.DeepEqual(oldPod.Spec, newPod.Spec) {\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// At this point we know that something in .metadata or .spec has changed, so we must proceed to sync the pod.\n\t\t\t\tk8sQ.AddRateLimited(key)\n\t\t},\n\t\tDeleteFunc: func(pod interface{}) {\n\t\t\tif key, err := cache.DeletionHandlingMetaNamespaceKeyFunc(pod); err != nil {\n\t\t\t\tlog.L.Error(err)\n\t\t\t} else {\n\t\t\t\tk8sQ.AddRateLimited(key)\n\t\t\t}\n\t\t},\n\t})\n\n\tpc.deleteDanglingPods(ctx, podSyncWorkers)\n\n\tfor id := 0; id &lt; podSyncWorkers; id++ {\n\t\tgo wait.Until(func() {\n\t\t\t// Use the worker&#39;s &quot;index&quot; as its ID so we can use it for tracing.\n\t\t\tpc.runWorker(ctx, strconv.Itoa(id), k8sQ)\n\t\t}, time.Second, ctx.Done())\n\t}\n\n\tclose(pc.ready)\n\n\t&lt;-ctx.Done()\n\n\treturn nil\n}\n\nfunc (pc *PodController) runWorker(ctx context.Context, workerId string, q workqueue.RateLimitingInterface) {\n\tfor pc.processNextWorkItem(ctx, workerId, q) {\n\t}\n}\n\nfunc (pc *PodController) processNextWorkItem(ctx context.Context, workerId string, q workqueue.RateLimitingInterface) bool {\n\treturn handleQueueItem(ctx, q, pc.syncHandler)\n}\n\nfunc (pc *PodController) syncHandler(ctx context.Context, key string) error {\n\tctx = span.WithField(ctx, &quot;key&quot;, key)\n\n\tnamespace, name, err := cache.SplitMetaNamespaceKey(key)\n\t// Get the Pod resource with this namespace/name.\n\tpod, err := pc.podsLister.Pods(namespace).Get(name)\n\tif err != nil {\n\t\tif !errors.IsNotFound(err) {\n\t\t\terr := pkgerrors.Wrapf(err, &quot;failed to fetch pod with key %q from lister&quot;, key)\n\t\t\tspan.SetStatus(err)\n\t\t\treturn err\n\t\t}\n\t\t// At this point we know the Pod resource doesn&#39;t exist, which most probably means it was deleted.\n\t\t// Hence, we must delete it from the provider if it still exists there.\n\t\tif err := pc.deletePod(ctx, namespace, name); err != nil {\n\t\t\terr := pkgerrors.Wrapf(err, &quot;failed to delete pod %q in the provider&quot;, loggablePodNameFromCoordinates(namespace, name))\n\t\t\tspan.SetStatus(err)\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t}\n\t// At this point we know the Pod resource has either been created or updated (which includes being marked for deletion).\n\treturn pc.syncPodInProvider(ctx, pod)\n}\n\nfunc (pc *PodController) syncPodInProvider(ctx context.Context, pod *corev1.Pod) error {\n\tctx, span := trace.StartSpan(ctx, &quot;syncPodInProvider&quot;)\n\tdefer span.End()\n\n\t// Add the pod&#39;s attributes to the current span.\n\tctx = addPodAttributes(ctx, span, pod)\n\n\t// Check whether the pod has been marked for deletion.\n\t// If it does, guarantee it is deleted in the provider and Kubernetes.\n\tif pod.DeletionTimestamp != nil {\n\t\tif err := pc.deletePod(ctx, pod.Namespace, pod.Name); err != nil {\n\t\t\terr := pkgerrors.Wrapf(err, &quot;failed to delete pod %q in the provider&quot;, loggablePodName(pod))\n\t\t\tspan.SetStatus(err)\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t}\n\n\t// Ignore the pod if it is in the &quot;Failed&quot; or &quot;Succeeded&quot; state.\n\tif pod.Status.Phase == corev1.PodFailed || pod.Status.Phase == corev1.PodSucceeded {\n\t\tlog.G(ctx).Warnf(&quot;skipping sync of pod %q in %q phase&quot;, loggablePodName(pod), pod.Status.Phase)\n\t\treturn nil\n\t}\n\n\t// Create or update the pod in the provider.\n\tif err := pc.createOrUpdatePod(ctx, pod); err != nil {\n\t\terr := pkgerrors.Wrapf(err, &quot;failed to sync pod %q in the provider&quot;, loggablePodName(pod))\n\t\tspan.SetStatus(err)\n\t\treturn err\n\t}\n\treturn nil\n}</code></pre></div>\n<p>启动一个 HTTPServer</p>\n<div class=\"gatsby-highlight\" data-language=\"golang\"><pre class=\"language-golang\"><code class=\"language-golang\">cancelHTTP, err := setupHTTPServer(ctx, p, apiConfig)\nif err != nil {\n    return err\n}\ndefer cancelHTTP()\n\n// 其中pod的handler，获取运行的pod、获取容器日志、执行命令\nfunc PodHandler(p PodHandlerConfig, debug bool) http.Handler {\n\tr := mux.NewRouter()\n\n\t// This matches the behaviour in the reference kubelet\n\tr.StrictSlash(true)\n\tif debug {\n\t\tr.HandleFunc(&quot;/runningpods/&quot;, HandleRunningPods(p.GetPods)).Methods(&quot;GET&quot;)\n\t}\n\tr.HandleFunc(&quot;/containerLogs/{namespace}/{pod}/{container}&quot;, HandleContainerLogs(p.GetContainerLogs)).Methods(&quot;GET&quot;)\n\tr.HandleFunc(&quot;/exec/{namespace}/{pod}/{container}&quot;, HandleContainerExec(p.RunInContainer)).Methods(&quot;POST&quot;)\n\tr.NotFoundHandler = http.HandlerFunc(NotFound)\n\treturn r\n}\n\n// PodStatsSummaryHandler 提供了/stats/summary，用于获取pod 的status，这是kubelet提供的一个能力。\nfunc PodStatsSummaryHandler(f PodStatsSummaryHandlerFunc) http.Handler {\n\tif f == nil {\n\t\treturn http.HandlerFunc(NotImplemented)\n\t}\n\n\tr := mux.NewRouter()\n\n\tconst summaryRoute = &quot;/stats/summary&quot;\n\th := HandlePodStatsSummary(f)\n\n\tr.Handle(summaryRoute, ochttp.WithRouteTag(h, &quot;PodStatsSummaryHandler&quot;)).Methods(&quot;GET&quot;)\n\tr.Handle(summaryRoute+&quot;/&quot;, ochttp.WithRouteTag(h, &quot;PodStatsSummaryHandler&quot;)).Methods(&quot;GET&quot;)\n\n\tr.NotFoundHandler = http.HandlerFunc(NotFound)\n\treturn r\n}</code></pre></div>\n<h1 id=\"限制\" style=\"position:relative;\"><a href=\"#%E9%99%90%E5%88%B6\" aria-label=\"限制 permalink\" class=\"toc-anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>限制</h1>\n<ol>\n<li>不支持 DaemonSet，比如说日志采集的 fluentd</li>\n<li>不支持 HostPath 的挂载</li>\n<li>不支持 HostNetwork</li>\n<li>不支持 NodePort 的 Service</li>\n</ol>\n<h1 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"toc-anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h1>\n<ul>\n<li><a href=\"https://virtual-kubelet.io/\">https://virtual-kubelet.io/</a></li>\n<li><a href=\"https://github.com/virtual-kubelet/virtual-kubelet\">https://github.com/virtual-kubelet/virtual-kubelet</a></li>\n<li><a href=\"https://github.com/virtual-kubelet/aws-fargate/blob/master/provider.go\">https://github.com/virtual-kubelet/aws-fargate/blob/master/provider.go</a></li>\n<li><a href=\"https://github.com/virtual-kubelet/nomad/blob/master/nomad.go\">https://github.com/virtual-kubelet/nomad/blob/master/nomad.go</a></li>\n<li><a href=\"https://github.com/virtual-kubelet/alibabacloud-eci/blob/master/eci.go\">https://github.com/virtual-kubelet/alibabacloud-eci/blob/master/eci.go</a></li>\n<li></li>\n</ul>","tableOfContents":"<ul>\n<li>\n<p><a href=\"#%E5%89%8D%E8%A8%80\">前言</a></p>\n</li>\n<li>\n<p><a href=\"#kubelet\">Kubelet</a></p>\n</li>\n<li>\n<p><a href=\"#virtual-kubelet\">Virtual Kubelet</a></p>\n<ul>\n<li><a href=\"#eci\">ECI</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%E9%99%90%E5%88%B6\">限制</a></p>\n</li>\n<li>\n<p><a href=\"#reference\">Reference</a></p>\n</li>\n</ul>","frontmatter":{"title":"virtual-kubelet","date":"2022-08-16"}}},"pageContext":{"slug":"/2022-08-16-virtual-kubelet"}},"staticQueryHashes":["3649515864"],"slicesMap":{}}